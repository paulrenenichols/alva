name: Deploy to Staging

on:
  push:
    branches: [staging]
    # Triggers when PR is merged to staging (direct pushes prevented by branch protection)
  workflow_dispatch: # Allow manual trigger

env:
  AWS_REGION: us-east-1
  NODE_VERSION: '20'
  STAGING_DOMAIN: staging.alva.paulrenenichols.com

jobs:
  build-and-push-images:
    name: Build and Push Docker Images
    runs-on: ubuntu-latest
    # Runs on push to staging (PR merges trigger push events)
    # or manual workflow dispatch
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Get AWS Account ID
        id: get-account-id
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV
          echo "ECR_REGISTRY=$ACCOUNT_ID.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com" >> $GITHUB_ENV

      - name: Build and push Web image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./apps/web/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REGISTRY }}/alva-web:latest
            ${{ env.ECR_REGISTRY }}/alva-web:${{ github.sha }}
          build-args: |
            NEXT_PUBLIC_AUTH_URL=http://auth.${{ env.STAGING_DOMAIN }}
            NEXT_PUBLIC_API_URL=http://api.${{ env.STAGING_DOMAIN }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push API image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./apps/api/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REGISTRY }}/alva-api:latest
            ${{ env.ECR_REGISTRY }}/alva-api:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push Auth image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./apps/auth/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REGISTRY }}/alva-auth:latest
            ${{ env.ECR_REGISTRY }}/alva-auth:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push Admin image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./apps/admin/Dockerfile
          push: true
          tags: |
            ${{ env.ECR_REGISTRY }}/alva-admin:latest
            ${{ env.ECR_REGISTRY }}/alva-admin:${{ github.sha }}
          build-args: |
            NEXT_PUBLIC_AUTH_URL=http://auth.${{ env.STAGING_DOMAIN }}
            NEXT_PUBLIC_API_URL=http://api.${{ env.STAGING_DOMAIN }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy-infrastructure:
    name: Deploy CDK Infrastructure
    runs-on: ubuntu-latest
    needs: build-and-push-images
    permissions:
      contents: read
      id-token: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: infrastructure/package-lock.json

      - name: Install CDK CLI globally
        run: npm install -g aws-cdk

      - name: Install CDK dependencies
        working-directory: ./infrastructure
        run: npm ci

      - name: Get AWS Account ID
        id: get-account-id
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV
          echo "CDK_DEFAULT_ACCOUNT=$ACCOUNT_ID" >> $GITHUB_ENV
          echo "CDK_DEFAULT_REGION=${{ env.AWS_REGION }}" >> $GITHUB_ENV

      - name: Build CDK
        working-directory: ./infrastructure
        run: npm run build

      - name: CDK Synth (verify)
        working-directory: ./infrastructure
        run: |
          export CDK_DEFAULT_ACCOUNT=${{ env.AWS_ACCOUNT_ID }}
          export CDK_DEFAULT_REGION=${{ env.AWS_REGION }}
          cdk synth --all

      - name: CDK Deploy (all stacks)
        working-directory: ./infrastructure
        run: |
          export CDK_DEFAULT_ACCOUNT=${{ env.AWS_ACCOUNT_ID }}
          export CDK_DEFAULT_REGION=${{ env.AWS_REGION }}
          cdk deploy --all --require-approval never

  run-migrations:
    name: Run Database Migrations
    runs-on: ubuntu-latest
    needs: deploy-infrastructure
    permissions:
      contents: read
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get latest auth task definition
        id: get-task-def
        run: |
          # Get auth service name dynamically (CDK generates names with random suffixes)
          AUTH_SERVICE=$(aws ecs list-services \
            --cluster alva-staging-cluster \
            --region ${{ env.AWS_REGION }} \
            --query 'serviceArns[?contains(@, `auth`)] | [0]' \
            --output text | xargs basename)

          TASK_DEF=$(aws ecs describe-services \
            --cluster alva-staging-cluster \
            --services "$AUTH_SERVICE" \
            --region ${{ env.AWS_REGION }} \
            --query 'services[0].taskDefinition' \
            --output text)

          echo "TASK_DEF=$TASK_DEF" >> $GITHUB_ENV
          echo "Auth service: $AUTH_SERVICE"
          echo "Task definition: $TASK_DEF"

      - name: Get ECS service network configuration
        id: get-network-config
        run: |
          # Get subnet IDs from the auth service (any service will work, they're all in the same VPC)
          SERVICE_ARN=$(aws ecs list-services \
            --cluster alva-staging-cluster \
            --region ${{ env.AWS_REGION }} \
            --query 'serviceArns[0]' \
            --output text)

          SUBNETS=$(aws ecs describe-services \
            --cluster alva-staging-cluster \
            --services $(basename $SERVICE_ARN) \
            --region ${{ env.AWS_REGION }} \
            --query 'services[0].networkConfiguration.awsvpcConfiguration.subnets[]' \
            --output text | tr '\t' ',')

          SECURITY_GROUPS=$(aws ecs describe-services \
            --cluster alva-staging-cluster \
            --services $(basename $SERVICE_ARN) \
            --region ${{ env.AWS_REGION }} \
            --query 'services[0].networkConfiguration.awsvpcConfiguration.securityGroups[]' \
            --output text | tr '\t' ',')

          echo "SUBNETS=$SUBNETS" >> $GITHUB_ENV
          echo "SECURITY_GROUPS=$SECURITY_GROUPS" >> $GITHUB_ENV
          echo "Subnets: $SUBNETS"
          echo "Security Groups: $SECURITY_GROUPS"

      - name: Run database migrations
        run: |
          echo "Running database migrations..."

          # Create migration command that runs SQL migrations
          # Note: This is a simplified migration script. In the future, we should
          # include migration files in the Docker image and use drizzle-kit migrate
          MIGRATION_CMD='node -e "
          const {Pool}=require(\"pg\");
          const url=`postgresql://${process.env.DATABASE_USERNAME}:${process.env.DATABASE_PASSWORD}@${process.env.DATABASE_ENDPOINT}:5432/alva`;
          const pool=new Pool({connectionString:url,ssl:{rejectUnauthorized:false}});
          (async()=>{
            try{
              const checkTable=async(tableName)=>{
                const result=await pool.query(`SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_schema = \"public\" AND table_name = $1) as exists`, [tableName]);
                return result.rows[0].exists;
              };
              const addConstraint=async(sql)=>{
                try{await pool.query(sql);}catch(e){if(!e.message.includes(\"already exists\"))throw e;}
              };
              if(!(await checkTable(\"admin_users\"))){
                console.log(\"Creating admin_users table...\");
                await pool.query(`CREATE TABLE admin_users (id uuid PRIMARY KEY DEFAULT gen_random_uuid() NOT NULL, email varchar(255) NOT NULL, password_hash varchar(255) NOT NULL, email_verified boolean DEFAULT true, must_reset_password boolean DEFAULT false, created_at timestamp DEFAULT now() NOT NULL, updated_at timestamp DEFAULT now() NOT NULL, CONSTRAINT admin_users_email_unique UNIQUE(email))`);
                await pool.query(`CREATE TABLE admin_roles (id uuid PRIMARY KEY DEFAULT gen_random_uuid() NOT NULL, name varchar(50) NOT NULL, description varchar(255), created_at timestamp DEFAULT now() NOT NULL, CONSTRAINT admin_roles_name_unique UNIQUE(name))`);
                await pool.query(`CREATE TABLE admin_user_roles (id uuid PRIMARY KEY DEFAULT gen_random_uuid() NOT NULL, admin_user_id uuid NOT NULL, role_id uuid NOT NULL, created_at timestamp DEFAULT now() NOT NULL)`);
                await pool.query(`CREATE TABLE admin_password_reset_tokens (id uuid PRIMARY KEY DEFAULT gen_random_uuid() NOT NULL, token varchar(255) NOT NULL, admin_user_id uuid NOT NULL, expires_at timestamp NOT NULL, used_at timestamp, created_at timestamp DEFAULT now() NOT NULL, CONSTRAINT admin_password_reset_tokens_token_unique UNIQUE(token))`);
                await addConstraint(`ALTER TABLE admin_user_roles ADD CONSTRAINT admin_user_roles_admin_user_id_admin_users_id_fk FOREIGN KEY (admin_user_id) REFERENCES public.admin_users(id) ON DELETE cascade ON UPDATE no action`);
                await addConstraint(`ALTER TABLE admin_user_roles ADD CONSTRAINT admin_user_roles_role_id_admin_roles_id_fk FOREIGN KEY (role_id) REFERENCES public.admin_roles(id) ON DELETE cascade ON UPDATE no action`);
                await addConstraint(`ALTER TABLE admin_password_reset_tokens ADD CONSTRAINT admin_password_reset_tokens_admin_user_id_admin_users_id_fk FOREIGN KEY (admin_user_id) REFERENCES public.admin_users(id) ON DELETE cascade ON UPDATE no action`);
                console.log(\"✅ Migrations completed\");
              }else{
                console.log(\"✅ Tables already exist, skipping migrations\");
              }
              await pool.end();
              process.exit(0);
            }catch(e){
              console.error(\"❌ Migration failed:\",e.message);
              console.error(e.stack);
              await pool.end();
              process.exit(1);
            }
          })();
          "'

          TASK_ARN=$(aws ecs run-task \
            --cluster alva-staging-cluster \
            --task-definition ${{ env.TASK_DEF }} \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ env.SUBNETS }}],securityGroups=[${{ env.SECURITY_GROUPS }}],assignPublicIp=DISABLED}" \
            --overrides "{\"containerOverrides\":[{\"name\":\"authContainer\",\"command\":[\"sh\",\"-c\",\"$MIGRATION_CMD\"]}]}" \
            --region ${{ env.AWS_REGION }} \
            --query 'tasks[0].taskArn' \
            --output text)

          echo "Migration task ARN: $TASK_ARN"
          echo "TASK_ARN=$TASK_ARN" >> $GITHUB_ENV

      - name: Wait for migrations to complete
        run: |
          echo "Waiting for migration task to complete..."
          TIMEOUT=300  # 5 minutes
          ELAPSED=0
          while [ $ELAPSED -lt $TIMEOUT ]; do
            STATUS=$(aws ecs describe-tasks \
              --cluster alva-staging-cluster \
              --tasks ${{ env.TASK_ARN }} \
              --region ${{ env.AWS_REGION }} \
              --query 'tasks[0].lastStatus' \
              --output text 2>/dev/null || echo "PENDING")
            
            if [ "$STATUS" = "STOPPED" ]; then
              EXIT_CODE=$(aws ecs describe-tasks \
                --cluster alva-staging-cluster \
                --tasks ${{ env.TASK_ARN }} \
                --region ${{ env.AWS_REGION }} \
                --query 'tasks[0].containers[0].exitCode' \
                --output text)
              
              if [ "$EXIT_CODE" = "0" ]; then
                echo "✅ Migrations completed successfully!"
                exit 0
              else
                echo "❌ Migration task failed with exit code: $EXIT_CODE"
                
                # Get logs for debugging
                TASK_ID=$(echo ${{ env.TASK_ARN }} | cut -d'/' -f3)
                echo "Checking logs for task: $TASK_ID"
                aws logs filter-log-events \
                  --log-group-name /ecs/alva-staging \
                  --log-stream-name-prefix "alva-auth/authContainer/$TASK_ID" \
                  --region ${{ env.AWS_REGION }} \
                  --output json \
                  | jq -r '.events[] | .message' \
                  | tail -20
                
                exit 1
              fi
            fi
            
            echo "Migration status: $STATUS (${ELAPSED}s elapsed)..."
            sleep 10
            ELAPSED=$((ELAPSED + 10))
          done

          echo "❌ Migration task timed out after $TIMEOUT seconds"
          exit 1

  seed-database:
    name: Seed Database (Roles and Admin Users)
    runs-on: ubuntu-latest
    needs: run-migrations
    permissions:
      contents: read
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get latest auth task definition
        id: get-task-def
        run: |
          AUTH_SERVICE=$(aws ecs list-services \
            --cluster alva-staging-cluster \
            --region ${{ env.AWS_REGION }} \
            --query 'serviceArns[?contains(@, `auth`)] | [0]' \
            --output text | xargs basename)

          TASK_DEF=$(aws ecs describe-services \
            --cluster alva-staging-cluster \
            --services "$AUTH_SERVICE" \
            --region ${{ env.AWS_REGION }} \
            --query 'services[0].taskDefinition' \
            --output text)

          echo "TASK_DEF=$TASK_DEF" >> $GITHUB_ENV
          echo "Auth service: $AUTH_SERVICE"
          echo "Task definition: $TASK_DEF"

      - name: Get ECS service network configuration
        id: get-network-config
        run: |
          SERVICE_ARN=$(aws ecs list-services \
            --cluster alva-staging-cluster \
            --region ${{ env.AWS_REGION }} \
            --query 'serviceArns[0]' \
            --output text)

          SUBNETS=$(aws ecs describe-services \
            --cluster alva-staging-cluster \
            --services $(basename $SERVICE_ARN) \
            --region ${{ env.AWS_REGION }} \
            --query 'services[0].networkConfiguration.awsvpcConfiguration.subnets[]' \
            --output text | tr '\t' ',')

          SECURITY_GROUPS=$(aws ecs describe-services \
            --cluster alva-staging-cluster \
            --services $(basename $SERVICE_ARN) \
            --region ${{ env.AWS_REGION }} \
            --query 'services[0].networkConfiguration.awsvpcConfiguration.securityGroups[]' \
            --output text | tr '\t' ',')

          echo "SUBNETS=$SUBNETS" >> $GITHUB_ENV
          echo "SECURITY_GROUPS=$SECURITY_GROUPS" >> $GITHUB_ENV

      - name: Upload seed script to S3
        run: |
          echo "Uploading seed script to S3..."
          aws s3 cp tools/scripts/seed-admins-staging.js s3://alva-staging-temp/seed-admins-staging.js \
            --region ${{ env.AWS_REGION }} || \
          aws s3 mb s3://alva-staging-temp --region ${{ env.AWS_REGION }}
          aws s3 cp tools/scripts/seed-admins-staging.js s3://alva-staging-temp/seed-admins-staging.js \
            --region ${{ env.AWS_REGION }}

      - name: Run database seeding
        run: |
          echo "Running database seeding (idempotent - checks if users exist first)..."

          PRESIGNED_URL=$(aws s3 presign s3://alva-staging-temp/seed-admins-staging.js \
            --expires-in 300 \
            --region ${{ env.AWS_REGION }})

          TASK_ARN=$(aws ecs run-task \
            --cluster alva-staging-cluster \
            --task-definition ${{ env.TASK_DEF }} \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ env.SUBNETS }}],securityGroups=[${{ env.SECURITY_GROUPS }}],assignPublicIp=DISABLED}" \
            --overrides "{\"containerOverrides\":[{\"name\":\"authContainer\",\"command\":[\"sh\",\"-c\",\"node -e \\\"const https=require('https');https.get('$PRESIGNED_URL',(r)=>{let d='';r.on('data',(c)=>{d+=c});r.on('end',()=>{eval(d)});}).on('error',(e)=>{console.error(e);process.exit(1)});\\\"\"]}]}" \
            --region ${{ env.AWS_REGION }} \
            --query 'tasks[0].taskArn' \
            --output text)

          echo "Seed task ARN: $TASK_ARN"
          echo "TASK_ARN=$TASK_ARN" >> $GITHUB_ENV

      - name: Wait for seeding to complete
        run: |
          echo "Waiting for seed task to complete..."
          TIMEOUT=300
          ELAPSED=0
          while [ $ELAPSED -lt $TIMEOUT ]; do
            STATUS=$(aws ecs describe-tasks \
              --cluster alva-staging-cluster \
              --tasks ${{ env.TASK_ARN }} \
              --region ${{ env.AWS_REGION }} \
              --query 'tasks[0].lastStatus' \
              --output text 2>/dev/null || echo "PENDING")

            if [ "$STATUS" = "STOPPED" ]; then
              EXIT_CODE=$(aws ecs describe-tasks \
                --cluster alva-staging-cluster \
                --tasks ${{ env.TASK_ARN }} \
                --region ${{ env.AWS_REGION }} \
                --query 'tasks[0].containers[0].exitCode' \
                --output text)

              if [ "$EXIT_CODE" = "0" ]; then
                echo "✅ Seeding completed successfully!"
                exit 0
              else
                echo "❌ Seed task failed with exit code: $EXIT_CODE"

                TASK_ID=$(echo ${{ env.TASK_ARN }} | cut -d'/' -f3)
                echo "Checking logs for task: $TASK_ID"
                aws logs filter-log-events \
                  --log-group-name /ecs/alva-staging \
                  --log-stream-name-prefix "alva-auth/authContainer/$TASK_ID" \
                  --region ${{ env.AWS_REGION }} \
                  --output json \
                  | jq -r '.events[] | .message' \
                  | tail -20

                exit 1
              fi
            fi

            echo "Seed status: $STATUS (${ELAPSED}s elapsed)..."
            sleep 10
            ELAPSED=$((ELAPSED + 10))
          done

          echo "❌ Seed task timed out after $TIMEOUT seconds"
          exit 1

  update-services:
    name: Update ECS Services
    runs-on: ubuntu-latest
    needs: seed-database
    permissions:
      contents: read
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Force new deployment of ECS services
        run: |
          # Update all services to use new images
          aws ecs update-service \
            --cluster alva-staging-cluster \
            --service alva-web \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }}

          aws ecs update-service \
            --cluster alva-staging-cluster \
            --service alva-api \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }}

          aws ecs update-service \
            --cluster alva-staging-cluster \
            --service alva-auth \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }}

          aws ecs update-service \
            --cluster alva-staging-cluster \
            --service alva-admin \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }}

      - name: Wait for services to stabilize
        run: |
          echo "Waiting for services to stabilize..."
          for service in alva-web alva-api alva-auth alva-admin; do
            aws ecs wait services-stable \
              --cluster alva-staging-cluster \
              --services $service \
              --region ${{ env.AWS_REGION }}
            echo "$service is stable"
          done
